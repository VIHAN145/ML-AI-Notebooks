{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning-Iris.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUZjPnVXGz0Z"
      },
      "source": [
        "# The Iris Dataset\n",
        "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
        "\n",
        "The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMbmpriavLE9"
      },
      "source": [
        "### Specifying the TensorFlow version\n",
        "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu8bUU__oa7h"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLz1Ckvfvn6D"
      },
      "source": [
        "### Import TensorFlow\n",
        "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWrzVTLOvn6M",
        "outputId": "3c2098ee-7c15-4427-b4ff-5a32fcd37a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uYeJgkNuXNC"
      },
      "source": [
        "### Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcASNsewsfQX"
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-vVQBBqg7DI"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE0EDKvQhEIe"
      },
      "source": [
        "### Import dataset\n",
        "- Import iris dataset\n",
        "- Import the dataset using sklearn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOOWpD26Haq3",
        "outputId": "7b47e97f-42e2-4e91-e672-0da943c404bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glOP9lh6zz4-"
      },
      "source": [
        "import numpy as np\n",
        "from pandas import read_csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oXKDAMlzIwv",
        "outputId": "4641e7da-a533-4f88-dcc6-52c08b139992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "\n",
        "DF = read_csv(\"/content/drive/My Drive/PGPAIML/Inclass Lab/iris.csv\", delimiter=\",\", header=None)\n",
        "dataset = dataframe.values   #Numpy Array \n",
        "dataframe.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Id</td>\n",
              "      <td>SepalLengthCm</td>\n",
              "      <td>SepalWidthCm</td>\n",
              "      <td>PetalLengthCm</td>\n",
              "      <td>PetalWidthCm</td>\n",
              "      <td>Species</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0              1             2              3             4            5\n",
              "0  Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
              "1   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
              "2   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
              "3   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
              "4   4            4.6           3.1            1.5           0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAj9MEEczv2d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta8YqInTh5v5"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HERt3drbhX0i"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- you can get the features using .data method\n",
        "- you can get the features using .target method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cV-_qHAHyvE"
      },
      "source": [
        "from sklearn import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C72ypjJ52TlH"
      },
      "source": [
        "iris = datasets.load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC563Pmd29Yr"
      },
      "source": [
        "X = iris.data[:, :5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zk7bDH34PNn"
      },
      "source": [
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zeareNu4Uxi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m1UFC7W4ZMg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg1A2lkUjFak"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YErwYLCH0N_"
      },
      "source": [
        "### Create train and test data\n",
        "- use train_test_split to get train and test set\n",
        "- set a random_state\n",
        "- test_size: 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYKNJL85h7pQ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x,test_x,train_y,test_y=train_test_split(X,y,test_size=.30,random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0KVP17Ozaix"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIjqxbhWv1zv"
      },
      "source": [
        "### One-hot encode the labels\n",
        "- convert class vectors (integers) to binary class matrix\n",
        "- convert labels\n",
        "- number of classes: 3\n",
        "- we are doing this to use categorical_crossentropy as loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9vv-_gpyLY9"
      },
      "source": [
        "y_train = tf.keras.utils.to_categorical(train_y, num_classes=3)\n",
        "y_test = tf.keras.utils.to_categorical(test_y, num_classes=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovjLyYzWkO9s"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbIFzoPNSyYo"
      },
      "source": [
        "### Initialize a sequential model\n",
        "- Define a sequential model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FvSbf1UjHtl"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "model = tf.keras.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGMy999vlacX"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ibK5Jxm8iL"
      },
      "source": [
        "### Add a layer\n",
        "- Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3\n",
        "- Apply Softmax on Dense Layer outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZKrBNSRm_o9"
      },
      "source": [
        "model.add(tf.keras.layers.Dense(4, input_dim=4, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23VD3Iop7oGF"
      },
      "source": [
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4uiTH8plmNX"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJL8n8vcSyYz"
      },
      "source": [
        "### Compile the model\n",
        "- Use SGD as Optimizer\n",
        "- Use categorical_crossentropy as loss function\n",
        "- Use accuracy as metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc_-fjIEk1ve"
      },
      "source": [
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sihIGbRll_jT"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ZZCfNGlu0i"
      },
      "source": [
        "### Summarize the model\n",
        "- Check model layers\n",
        "- Understand number of trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elER3F_4ln8n",
        "outputId": "a54d8d74-5f65-4cab-d456-4785e76871fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_13 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 15        \n",
            "=================================================================\n",
            "Total params: 35\n",
            "Trainable params: 35\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PiP7j3Vmj4p"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWdbfFCXmCHt"
      },
      "source": [
        "### Fit the model\n",
        "- Give train data as training features and labels\n",
        "- Epochs: 100\n",
        "- Give validation data as testing features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO1c-5tjmBVZ",
        "outputId": "f5629cd6-de9a-4b6b-f9df-3628f512d75b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(train_x, y_train, validation_data=(test_x,y_test), epochs=100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 45 samples\n",
            "Epoch 1/100\n",
            "105/105 [==============================] - 0s 3ms/sample - loss: 0.6704 - accuracy: 0.6667 - val_loss: 0.6804 - val_accuracy: 0.6667\n",
            "Epoch 2/100\n",
            "105/105 [==============================] - 0s 190us/sample - loss: 0.6688 - accuracy: 0.6667 - val_loss: 0.6788 - val_accuracy: 0.6667\n",
            "Epoch 3/100\n",
            "105/105 [==============================] - 0s 181us/sample - loss: 0.6674 - accuracy: 0.6667 - val_loss: 0.6768 - val_accuracy: 0.6667\n",
            "Epoch 4/100\n",
            "105/105 [==============================] - 0s 181us/sample - loss: 0.6659 - accuracy: 0.6667 - val_loss: 0.6752 - val_accuracy: 0.6667\n",
            "Epoch 5/100\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.6646 - accuracy: 0.6667 - val_loss: 0.6737 - val_accuracy: 0.6667\n",
            "Epoch 6/100\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.6635 - accuracy: 0.6667 - val_loss: 0.6721 - val_accuracy: 0.6667\n",
            "Epoch 7/100\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.6622 - accuracy: 0.6667 - val_loss: 0.6709 - val_accuracy: 0.6667\n",
            "Epoch 8/100\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.6611 - accuracy: 0.6667 - val_loss: 0.6691 - val_accuracy: 0.6667\n",
            "Epoch 9/100\n",
            "105/105 [==============================] - 0s 204us/sample - loss: 0.6598 - accuracy: 0.6667 - val_loss: 0.6673 - val_accuracy: 0.6667\n",
            "Epoch 10/100\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.6583 - accuracy: 0.6667 - val_loss: 0.6655 - val_accuracy: 0.6667\n",
            "Epoch 11/100\n",
            "105/105 [==============================] - 0s 190us/sample - loss: 0.6569 - accuracy: 0.6667 - val_loss: 0.6634 - val_accuracy: 0.6667\n",
            "Epoch 12/100\n",
            "105/105 [==============================] - 0s 181us/sample - loss: 0.6553 - accuracy: 0.6667 - val_loss: 0.6616 - val_accuracy: 0.6667\n",
            "Epoch 13/100\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.6540 - accuracy: 0.6667 - val_loss: 0.6601 - val_accuracy: 0.6667\n",
            "Epoch 14/100\n",
            "105/105 [==============================] - 0s 181us/sample - loss: 0.6528 - accuracy: 0.6667 - val_loss: 0.6582 - val_accuracy: 0.6667\n",
            "Epoch 15/100\n",
            "105/105 [==============================] - 0s 175us/sample - loss: 0.6514 - accuracy: 0.6667 - val_loss: 0.6568 - val_accuracy: 0.6667\n",
            "Epoch 16/100\n",
            "105/105 [==============================] - 0s 213us/sample - loss: 0.6504 - accuracy: 0.6667 - val_loss: 0.6562 - val_accuracy: 0.6667\n",
            "Epoch 17/100\n",
            "105/105 [==============================] - 0s 225us/sample - loss: 0.6496 - accuracy: 0.6667 - val_loss: 0.6547 - val_accuracy: 0.6667\n",
            "Epoch 18/100\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.6486 - accuracy: 0.6667 - val_loss: 0.6531 - val_accuracy: 0.6667\n",
            "Epoch 19/100\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 0.6474 - accuracy: 0.6667 - val_loss: 0.6520 - val_accuracy: 0.6667\n",
            "Epoch 20/100\n",
            "105/105 [==============================] - 0s 190us/sample - loss: 0.6464 - accuracy: 0.6667 - val_loss: 0.6508 - val_accuracy: 0.6667\n",
            "Epoch 21/100\n",
            "105/105 [==============================] - 0s 213us/sample - loss: 0.6454 - accuracy: 0.6667 - val_loss: 0.6496 - val_accuracy: 0.6667\n",
            "Epoch 22/100\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 0.6444 - accuracy: 0.6667 - val_loss: 0.6484 - val_accuracy: 0.6667\n",
            "Epoch 23/100\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.6433 - accuracy: 0.6667 - val_loss: 0.6470 - val_accuracy: 0.6667\n",
            "Epoch 24/100\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.6423 - accuracy: 0.6667 - val_loss: 0.6460 - val_accuracy: 0.6667\n",
            "Epoch 25/100\n",
            "105/105 [==============================] - 0s 199us/sample - loss: 0.6415 - accuracy: 0.6667 - val_loss: 0.6448 - val_accuracy: 0.6667\n",
            "Epoch 26/100\n",
            "105/105 [==============================] - 0s 183us/sample - loss: 0.6406 - accuracy: 0.6667 - val_loss: 0.6439 - val_accuracy: 0.6667\n",
            "Epoch 27/100\n",
            "105/105 [==============================] - 0s 213us/sample - loss: 0.6398 - accuracy: 0.6667 - val_loss: 0.6432 - val_accuracy: 0.6667\n",
            "Epoch 28/100\n",
            "105/105 [==============================] - 0s 183us/sample - loss: 0.6390 - accuracy: 0.6667 - val_loss: 0.6424 - val_accuracy: 0.6667\n",
            "Epoch 29/100\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.6381 - accuracy: 0.6667 - val_loss: 0.6412 - val_accuracy: 0.6667\n",
            "Epoch 30/100\n",
            "105/105 [==============================] - 0s 183us/sample - loss: 0.6372 - accuracy: 0.6667 - val_loss: 0.6403 - val_accuracy: 0.6667\n",
            "Epoch 31/100\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.6364 - accuracy: 0.6667 - val_loss: 0.6394 - val_accuracy: 0.6667\n",
            "Epoch 32/100\n",
            "105/105 [==============================] - 0s 181us/sample - loss: 0.6356 - accuracy: 0.6667 - val_loss: 0.6386 - val_accuracy: 0.6667\n",
            "Epoch 33/100\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.6348 - accuracy: 0.6667 - val_loss: 0.6373 - val_accuracy: 0.6667\n",
            "Epoch 34/100\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.6339 - accuracy: 0.6667 - val_loss: 0.6362 - val_accuracy: 0.6667\n",
            "Epoch 35/100\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.6330 - accuracy: 0.6667 - val_loss: 0.6355 - val_accuracy: 0.6667\n",
            "Epoch 36/100\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.6323 - accuracy: 0.6667 - val_loss: 0.6346 - val_accuracy: 0.6667\n",
            "Epoch 37/100\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.6315 - accuracy: 0.6667 - val_loss: 0.6338 - val_accuracy: 0.6667\n",
            "Epoch 38/100\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.6308 - accuracy: 0.6667 - val_loss: 0.6329 - val_accuracy: 0.6667\n",
            "Epoch 39/100\n",
            "105/105 [==============================] - 0s 201us/sample - loss: 0.6301 - accuracy: 0.6667 - val_loss: 0.6319 - val_accuracy: 0.6667\n",
            "Epoch 40/100\n",
            "105/105 [==============================] - 0s 180us/sample - loss: 0.6293 - accuracy: 0.6667 - val_loss: 0.6309 - val_accuracy: 0.6667\n",
            "Epoch 41/100\n",
            "105/105 [==============================] - 0s 182us/sample - loss: 0.6285 - accuracy: 0.6667 - val_loss: 0.6299 - val_accuracy: 0.6667\n",
            "Epoch 42/100\n",
            "105/105 [==============================] - 0s 198us/sample - loss: 0.6278 - accuracy: 0.6667 - val_loss: 0.6292 - val_accuracy: 0.6667\n",
            "Epoch 43/100\n",
            "105/105 [==============================] - 0s 208us/sample - loss: 0.6271 - accuracy: 0.6667 - val_loss: 0.6283 - val_accuracy: 0.6667\n",
            "Epoch 44/100\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.6264 - accuracy: 0.6667 - val_loss: 0.6277 - val_accuracy: 0.6667\n",
            "Epoch 45/100\n",
            "105/105 [==============================] - 0s 208us/sample - loss: 0.6258 - accuracy: 0.6667 - val_loss: 0.6273 - val_accuracy: 0.6667\n",
            "Epoch 46/100\n",
            "105/105 [==============================] - 0s 200us/sample - loss: 0.6252 - accuracy: 0.6667 - val_loss: 0.6266 - val_accuracy: 0.6667\n",
            "Epoch 47/100\n",
            "105/105 [==============================] - 0s 179us/sample - loss: 0.6245 - accuracy: 0.6667 - val_loss: 0.6258 - val_accuracy: 0.6667\n",
            "Epoch 48/100\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.6237 - accuracy: 0.6667 - val_loss: 0.6253 - val_accuracy: 0.6667\n",
            "Epoch 49/100\n",
            "105/105 [==============================] - 0s 197us/sample - loss: 0.6231 - accuracy: 0.6667 - val_loss: 0.6244 - val_accuracy: 0.6667\n",
            "Epoch 50/100\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.6224 - accuracy: 0.6667 - val_loss: 0.6238 - val_accuracy: 0.6667\n",
            "Epoch 51/100\n",
            "105/105 [==============================] - 0s 184us/sample - loss: 0.6218 - accuracy: 0.6667 - val_loss: 0.6231 - val_accuracy: 0.6667\n",
            "Epoch 52/100\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.6213 - accuracy: 0.6667 - val_loss: 0.6225 - val_accuracy: 0.6667\n",
            "Epoch 53/100\n",
            "105/105 [==============================] - 0s 175us/sample - loss: 0.6205 - accuracy: 0.6667 - val_loss: 0.6220 - val_accuracy: 0.6667\n",
            "Epoch 54/100\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.6200 - accuracy: 0.6667 - val_loss: 0.6211 - val_accuracy: 0.6667\n",
            "Epoch 55/100\n",
            "105/105 [==============================] - 0s 206us/sample - loss: 0.6195 - accuracy: 0.6667 - val_loss: 0.6207 - val_accuracy: 0.6667\n",
            "Epoch 56/100\n",
            "105/105 [==============================] - 0s 183us/sample - loss: 0.6188 - accuracy: 0.6667 - val_loss: 0.6200 - val_accuracy: 0.6667\n",
            "Epoch 57/100\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.6181 - accuracy: 0.6667 - val_loss: 0.6196 - val_accuracy: 0.6667\n",
            "Epoch 58/100\n",
            "105/105 [==============================] - 0s 228us/sample - loss: 0.6176 - accuracy: 0.6667 - val_loss: 0.6191 - val_accuracy: 0.6667\n",
            "Epoch 59/100\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 0.6170 - accuracy: 0.6667 - val_loss: 0.6185 - val_accuracy: 0.6667\n",
            "Epoch 60/100\n",
            "105/105 [==============================] - 0s 169us/sample - loss: 0.6164 - accuracy: 0.6667 - val_loss: 0.6179 - val_accuracy: 0.6667\n",
            "Epoch 61/100\n",
            "105/105 [==============================] - 0s 182us/sample - loss: 0.6159 - accuracy: 0.6667 - val_loss: 0.6173 - val_accuracy: 0.6667\n",
            "Epoch 62/100\n",
            "105/105 [==============================] - 0s 170us/sample - loss: 0.6153 - accuracy: 0.6667 - val_loss: 0.6167 - val_accuracy: 0.6667\n",
            "Epoch 63/100\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.6148 - accuracy: 0.6667 - val_loss: 0.6162 - val_accuracy: 0.6667\n",
            "Epoch 64/100\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 0.6142 - accuracy: 0.6667 - val_loss: 0.6156 - val_accuracy: 0.6667\n",
            "Epoch 65/100\n",
            "105/105 [==============================] - 0s 209us/sample - loss: 0.6137 - accuracy: 0.6667 - val_loss: 0.6148 - val_accuracy: 0.6667\n",
            "Epoch 66/100\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.6130 - accuracy: 0.6667 - val_loss: 0.6144 - val_accuracy: 0.6667\n",
            "Epoch 67/100\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.6125 - accuracy: 0.6667 - val_loss: 0.6138 - val_accuracy: 0.6667\n",
            "Epoch 68/100\n",
            "105/105 [==============================] - 0s 205us/sample - loss: 0.6120 - accuracy: 0.6667 - val_loss: 0.6132 - val_accuracy: 0.6667\n",
            "Epoch 69/100\n",
            "105/105 [==============================] - 0s 192us/sample - loss: 0.6115 - accuracy: 0.6667 - val_loss: 0.6124 - val_accuracy: 0.6667\n",
            "Epoch 70/100\n",
            "105/105 [==============================] - 0s 190us/sample - loss: 0.6109 - accuracy: 0.6667 - val_loss: 0.6119 - val_accuracy: 0.6667\n",
            "Epoch 71/100\n",
            "105/105 [==============================] - 0s 189us/sample - loss: 0.6104 - accuracy: 0.6667 - val_loss: 0.6115 - val_accuracy: 0.6667\n",
            "Epoch 72/100\n",
            "105/105 [==============================] - 0s 192us/sample - loss: 0.6099 - accuracy: 0.6667 - val_loss: 0.6110 - val_accuracy: 0.6667\n",
            "Epoch 73/100\n",
            "105/105 [==============================] - 0s 191us/sample - loss: 0.6094 - accuracy: 0.6667 - val_loss: 0.6107 - val_accuracy: 0.6667\n",
            "Epoch 74/100\n",
            "105/105 [==============================] - 0s 170us/sample - loss: 0.6089 - accuracy: 0.6667 - val_loss: 0.6100 - val_accuracy: 0.6667\n",
            "Epoch 75/100\n",
            "105/105 [==============================] - 0s 190us/sample - loss: 0.6084 - accuracy: 0.6667 - val_loss: 0.6097 - val_accuracy: 0.6667\n",
            "Epoch 76/100\n",
            "105/105 [==============================] - 0s 227us/sample - loss: 0.6080 - accuracy: 0.6667 - val_loss: 0.6091 - val_accuracy: 0.6667\n",
            "Epoch 77/100\n",
            "105/105 [==============================] - 0s 212us/sample - loss: 0.6075 - accuracy: 0.6667 - val_loss: 0.6089 - val_accuracy: 0.6667\n",
            "Epoch 78/100\n",
            "105/105 [==============================] - 0s 214us/sample - loss: 0.6070 - accuracy: 0.6667 - val_loss: 0.6084 - val_accuracy: 0.6667\n",
            "Epoch 79/100\n",
            "105/105 [==============================] - 0s 193us/sample - loss: 0.6065 - accuracy: 0.6667 - val_loss: 0.6081 - val_accuracy: 0.6667\n",
            "Epoch 80/100\n",
            "105/105 [==============================] - 0s 181us/sample - loss: 0.6061 - accuracy: 0.6667 - val_loss: 0.6078 - val_accuracy: 0.6667\n",
            "Epoch 81/100\n",
            "105/105 [==============================] - 0s 206us/sample - loss: 0.6056 - accuracy: 0.6667 - val_loss: 0.6073 - val_accuracy: 0.6667\n",
            "Epoch 82/100\n",
            "105/105 [==============================] - 0s 201us/sample - loss: 0.6051 - accuracy: 0.6667 - val_loss: 0.6068 - val_accuracy: 0.6667\n",
            "Epoch 83/100\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 0.6048 - accuracy: 0.6667 - val_loss: 0.6060 - val_accuracy: 0.6667\n",
            "Epoch 84/100\n",
            "105/105 [==============================] - 0s 201us/sample - loss: 0.6042 - accuracy: 0.6667 - val_loss: 0.6057 - val_accuracy: 0.6667\n",
            "Epoch 85/100\n",
            "105/105 [==============================] - 0s 173us/sample - loss: 0.6037 - accuracy: 0.6667 - val_loss: 0.6050 - val_accuracy: 0.6667\n",
            "Epoch 86/100\n",
            "105/105 [==============================] - 0s 193us/sample - loss: 0.6032 - accuracy: 0.6667 - val_loss: 0.6045 - val_accuracy: 0.6667\n",
            "Epoch 87/100\n",
            "105/105 [==============================] - 0s 212us/sample - loss: 0.6029 - accuracy: 0.6667 - val_loss: 0.6040 - val_accuracy: 0.6667\n",
            "Epoch 88/100\n",
            "105/105 [==============================] - 0s 212us/sample - loss: 0.6023 - accuracy: 0.6667 - val_loss: 0.6035 - val_accuracy: 0.6667\n",
            "Epoch 89/100\n",
            "105/105 [==============================] - 0s 191us/sample - loss: 0.6018 - accuracy: 0.6667 - val_loss: 0.6030 - val_accuracy: 0.6667\n",
            "Epoch 90/100\n",
            "105/105 [==============================] - 0s 211us/sample - loss: 0.6014 - accuracy: 0.6667 - val_loss: 0.6025 - val_accuracy: 0.6667\n",
            "Epoch 91/100\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.6009 - accuracy: 0.6667 - val_loss: 0.6019 - val_accuracy: 0.6667\n",
            "Epoch 92/100\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.6006 - accuracy: 0.6667 - val_loss: 0.6015 - val_accuracy: 0.6667\n",
            "Epoch 93/100\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.6001 - accuracy: 0.6667 - val_loss: 0.6012 - val_accuracy: 0.6667\n",
            "Epoch 94/100\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.5996 - accuracy: 0.6667 - val_loss: 0.6008 - val_accuracy: 0.6667\n",
            "Epoch 95/100\n",
            "105/105 [==============================] - 0s 206us/sample - loss: 0.5992 - accuracy: 0.6667 - val_loss: 0.6003 - val_accuracy: 0.6667\n",
            "Epoch 96/100\n",
            "105/105 [==============================] - 0s 180us/sample - loss: 0.5987 - accuracy: 0.6667 - val_loss: 0.5999 - val_accuracy: 0.6667\n",
            "Epoch 97/100\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.5983 - accuracy: 0.6667 - val_loss: 0.5994 - val_accuracy: 0.6667\n",
            "Epoch 98/100\n",
            "105/105 [==============================] - 0s 186us/sample - loss: 0.5979 - accuracy: 0.6667 - val_loss: 0.5990 - val_accuracy: 0.6667\n",
            "Epoch 99/100\n",
            "105/105 [==============================] - 0s 189us/sample - loss: 0.5974 - accuracy: 0.6667 - val_loss: 0.5986 - val_accuracy: 0.6667\n",
            "Epoch 100/100\n",
            "105/105 [==============================] - 0s 188us/sample - loss: 0.5971 - accuracy: 0.6667 - val_loss: 0.5980 - val_accuracy: 0.6667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc87490bb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re9ItAR3yS3J"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liw0IFf9yVqH"
      },
      "source": [
        "### Make predictions\n",
        "- Predict labels on one row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5sBybi6mlLl",
        "outputId": "300e6869-7b54-4c89-b642-b89c851a59b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred=model.predict(test_x)\n",
        "out2 = []\n",
        "for val in y_pred:\n",
        "    out2.append(np.argmax(val))\n",
        "print(out2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSUgMq3m0bG7"
      },
      "source": [
        "### Compare the prediction with actual label\n",
        "- Print the same row as done in the previous step but of actual labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5WbwVPyz-qQ",
        "outputId": "f5632fcd-934f-44e6-aa57-d3947281a1b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(test_y,out2).reset_index()\n",
        "\n",
        "df.set_axis(['Predicted','Actual'],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: set_axis currently defaults to operating inplace.\n",
            "This will change in a future version of pandas, use inplace=True to avoid this warning.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iysBNvG7Eof6",
        "outputId": "21ac16eb-bfb9-4bd0-b938-f06ddd6f1922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Predicted  Actual\n",
              "0           0       0\n",
              "1           0       0\n",
              "2           2       2\n",
              "3           0       0\n",
              "4           0       0\n",
              "5           2       2\n",
              "6           0       0\n",
              "7           2       2\n",
              "8           2       2\n",
              "9           0       0\n",
              "10          0       0\n",
              "11          0       0\n",
              "12          0       0\n",
              "13          0       0\n",
              "14          2       1\n",
              "15          2       1\n",
              "16          0       0\n",
              "17          2       1\n",
              "18          2       2\n",
              "19          2       1\n",
              "20          2       1\n",
              "21          2       1\n",
              "22          2       2\n",
              "23          2       1\n",
              "24          2       1\n",
              "25          0       0\n",
              "26          0       0\n",
              "27          2       2\n",
              "28          0       0\n",
              "29          2       2\n",
              "30          2       2\n",
              "31          0       0\n",
              "32          0       1\n",
              "33          2       2\n",
              "34          2       1\n",
              "35          0       0\n",
              "36          2       2\n",
              "37          2       1\n",
              "38          2       1\n",
              "39          2       2\n",
              "40          2       1\n",
              "41          2       1\n",
              "42          2       2\n",
              "43          2       1\n",
              "44          0       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrTKwbgE7NFT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1UBYPNp5Tn1"
      },
      "source": [
        "# Stock prices dataset\n",
        "The data is of tock exchange's stock listings for each trading day of 2010 to 2016.\n",
        "\n",
        "## Description\n",
        "A brief description of columns.\n",
        "- open: The opening market price of the equity symbol on the date\n",
        "- high: The highest market price of the equity symbol on the date\n",
        "- low: The lowest recorded market price of the equity symbol on the date\n",
        "- close: The closing recorded price of the equity symbol on the date\n",
        "- symbol: Symbol of the listed company\n",
        "- volume: Total traded volume of the equity symbol on the date\n",
        "- date: Date of record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctH_ZW5g-M3g"
      },
      "source": [
        "### Specifying the TensorFlow version\n",
        "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQbdODpH-M3r"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFQWH1tj-M38"
      },
      "source": [
        "### Import TensorFlow\n",
        "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho5n-xhd-M3_",
        "outputId": "a33d46f5-342c-4286-c111-54391fef1343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgkl0qu6-M4F"
      },
      "source": [
        "### Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKgTyuA3-M4G"
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_88voqAH-O6J"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRHCeJqP-evf"
      },
      "source": [
        "### Load the data\n",
        "- load the csv file and read it using pandas\n",
        "- file name is prices.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKVH5v7r-RmC",
        "outputId": "52ab3fc9-776b-4a82-dd0a-c90b3e05d7f8",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# run this cell to upload file if you are using google colab\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3868bee7-930d-41c1-8787-857b445c13ae\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3868bee7-930d-41c1-8787-857b445c13ae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving prices.csv to prices.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gDC6cSW_FSK"
      },
      "source": [
        "df = read_csv(\"prices.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGKaT5n2N6Vi",
        "outputId": "6f09c965-ac40-476c-8d96-60196e472601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>symbol</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-01-05 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-01-06 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-01-07 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-08 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-01-11 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date symbol        open  ...         low        high     volume\n",
              "0  2016-01-05 00:00:00   WLTW  123.430000  ...  122.309998  126.250000  2163600.0\n",
              "1  2016-01-06 00:00:00   WLTW  125.239998  ...  119.940002  125.540001  2386400.0\n",
              "2  2016-01-07 00:00:00   WLTW  116.379997  ...  114.930000  119.739998  2489500.0\n",
              "3  2016-01-08 00:00:00   WLTW  115.480003  ...  113.500000  117.440002  2006300.0\n",
              "4  2016-01-11 00:00:00   WLTW  117.010002  ...  114.089996  117.330002  1408600.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlLKVPVH_BCT"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4BlzVA_gZd"
      },
      "source": [
        "### Drop columnns\n",
        "- drop \"date\" and \"symbol\" column from the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKEK8aEE_Csx"
      },
      "source": [
        "ndf = df.drop(['date','symbol'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPI0BBrjOHur",
        "outputId": "cd95593c-8787-4a33-d2da-42f207e18796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "ndf.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTPhO6v-AiZt"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsZXmF3NAkna"
      },
      "source": [
        "### Take initial rows\n",
        "- Take first 1000 rows from the data\n",
        "- This step is done to make the execution faster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKs04iIHAjxN"
      },
      "source": [
        "ndf = ndf.iloc[0:1000,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLYc6dS9OOIK",
        "outputId": "6bcb38f5-d049-488f-8964-0d153e9e266b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ndf.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vGtnapgBIJm"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8u_jlbABTip"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- Take \"open\", \"close\", \"low\", \"high\" columns as features\n",
        "- Take \"volume\" column as label\n",
        "- Normalize label column by dividing it with 1000000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQjCMzUXBJbg"
      },
      "source": [
        "X = ndf[['open','close','low','high']]\n",
        "y = ndf['volume']/1000000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTAKzlxZBz0z"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfY8Km1Zzyt2"
      },
      "source": [
        "### Convert data\n",
        "- Convert features and labels to numpy array\n",
        "- Convert their data type to \"float32\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko7nnQVbYENh"
      },
      "source": [
        "Xarr = X.values\n",
        "yarr = y.values\n",
        "Xarr32 =tf.cast(Xarr,tf.float32)\n",
        "yarr32 = tf.cast(Xarr,tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TWpN0nVTpUx"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ1FKEs-4btX"
      },
      "source": [
        "### Normalize data\n",
        "- Normalize features\n",
        "- Use tf.math.l2_normalize to normalize features\n",
        "- You can read more about it here https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0Tfe00X78wB"
      },
      "source": [
        "Xarr32 = tf.math.l2_normalize(Xarr32,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmXUGc2oTspa"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJelDMpzxs0L"
      },
      "source": [
        "### Define weight and bias\n",
        "- Initialize weight and bias with tf.zeros\n",
        "- tf.zeros is an initializer that generates tensors initialized to 0\n",
        "- Specify the value for shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o9RPWVTxs0O"
      },
      "source": [
        "W = tf.Variable( tf.zeros( [4,1] ) )\n",
        "B = tf.Variable( tf.zeros( [1] ) ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a0wr94aTyjg"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMXXYdOSxs0Q"
      },
      "source": [
        "### Get prediction\n",
        "- Define a function to get prediction\n",
        "- Approach: prediction = (X * W) + b; here is X is features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8Cty1y0xs0S"
      },
      "source": [
        "def frwrd_pass(X_train,W,B):\n",
        "    out = tf.matmul( X_train,W ) + B\n",
        "    return out\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQmS3Tauxs0V"
      },
      "source": [
        "### Calculate loss\n",
        "- Calculate loss using predictions\n",
        "- Define a function to calculate loss\n",
        "- We are calculating mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FRXmDd5xs0X"
      },
      "source": [
        "def loss(predicted_y, desired_y):\n",
        "  return tf.reduce_mean(tf.square(predicted_y - desired_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbBpnOtfT0wd"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkOzAUUsTmF_"
      },
      "source": [
        "### Define a function to train the model\n",
        "1.   Record all the mathematical steps to calculate Loss\n",
        "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
        "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R4uieGYLYtM"
      },
      "source": [
        "def train(X_train, Y_train, epochs):\n",
        "  optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    with tf.GradientTape() as tape: #tf.GradientTape records all operations in forward pass to apply gradient later\n",
        "      predicted = frwrd_pass( X_train,W,B )\n",
        "      curr_loss = loss(predicted, yarr32)\n",
        "    grads = tape.gradient( curr_loss, [W,B] )#return derivative of loss function wrt weight and bias\n",
        "    optimizer.apply_gradients(zip(grads, [W,B]),global_step=tf.compat.v1.train.get_or_create_global_step()) #applys Gradient Descent\n",
        "    print(\"Loss at step {:d}: {:.3f}\".format(i, loss(frwrd_pass( X_train,W,B ), Y_train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW4SEP8kT2ls"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeN0deOvT81N"
      },
      "source": [
        "### Train the model for 100 epochs \n",
        "- Observe the training loss at every iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjkn4gUgLevE",
        "outputId": "7660f432-dc94-4828-c5b1-20f708154272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(Xarr32,yarr,100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at step 0: 3553.912\n",
            "Loss at step 1: 3291.680\n",
            "Loss at step 2: 3050.008\n",
            "Loss at step 3: 2827.281\n",
            "Loss at step 4: 2622.017\n",
            "Loss at step 5: 2432.845\n",
            "Loss at step 6: 2258.503\n",
            "Loss at step 7: 2097.830\n",
            "Loss at step 8: 1949.752\n",
            "Loss at step 9: 1813.284\n",
            "Loss at step 10: 1687.515\n",
            "Loss at step 11: 1571.606\n",
            "Loss at step 12: 1464.784\n",
            "Loss at step 13: 1366.336\n",
            "Loss at step 14: 1275.607\n",
            "Loss at step 15: 1191.990\n",
            "Loss at step 16: 1114.930\n",
            "Loss at step 17: 1043.910\n",
            "Loss at step 18: 978.458\n",
            "Loss at step 19: 918.138\n",
            "Loss at step 20: 862.546\n",
            "Loss at step 21: 811.313\n",
            "Loss at step 22: 764.096\n",
            "Loss at step 23: 720.582\n",
            "Loss at step 24: 680.478\n",
            "Loss at step 25: 643.519\n",
            "Loss at step 26: 609.457\n",
            "Loss at step 27: 578.065\n",
            "Loss at step 28: 549.135\n",
            "Loss at step 29: 522.473\n",
            "Loss at step 30: 497.901\n",
            "Loss at step 31: 475.255\n",
            "Loss at step 32: 454.385\n",
            "Loss at step 33: 435.151\n",
            "Loss at step 34: 417.424\n",
            "Loss at step 35: 401.088\n",
            "Loss at step 36: 386.032\n",
            "Loss at step 37: 372.157\n",
            "Loss at step 38: 359.369\n",
            "Loss at step 39: 347.584\n",
            "Loss at step 40: 336.723\n",
            "Loss at step 41: 326.713\n",
            "Loss at step 42: 317.488\n",
            "Loss at step 43: 308.987\n",
            "Loss at step 44: 301.152\n",
            "Loss at step 45: 293.931\n",
            "Loss at step 46: 287.276\n",
            "Loss at step 47: 281.143\n",
            "Loss at step 48: 275.490\n",
            "Loss at step 49: 270.281\n",
            "Loss at step 50: 265.481\n",
            "Loss at step 51: 261.056\n",
            "Loss at step 52: 256.979\n",
            "Loss at step 53: 253.221\n",
            "Loss at step 54: 249.758\n",
            "Loss at step 55: 246.566\n",
            "Loss at step 56: 243.624\n",
            "Loss at step 57: 240.913\n",
            "Loss at step 58: 238.415\n",
            "Loss at step 59: 236.113\n",
            "Loss at step 60: 233.991\n",
            "Loss at step 61: 232.035\n",
            "Loss at step 62: 230.233\n",
            "Loss at step 63: 228.572\n",
            "Loss at step 64: 227.041\n",
            "Loss at step 65: 225.630\n",
            "Loss at step 66: 224.330\n",
            "Loss at step 67: 223.132\n",
            "Loss at step 68: 222.027\n",
            "Loss at step 69: 221.010\n",
            "Loss at step 70: 220.072\n",
            "Loss at step 71: 219.207\n",
            "Loss at step 72: 218.411\n",
            "Loss at step 73: 217.676\n",
            "Loss at step 74: 217.000\n",
            "Loss at step 75: 216.376\n",
            "Loss at step 76: 215.802\n",
            "Loss at step 77: 215.272\n",
            "Loss at step 78: 214.784\n",
            "Loss at step 79: 214.334\n",
            "Loss at step 80: 213.919\n",
            "Loss at step 81: 213.537\n",
            "Loss at step 82: 213.185\n",
            "Loss at step 83: 212.861\n",
            "Loss at step 84: 212.562\n",
            "Loss at step 85: 212.286\n",
            "Loss at step 86: 212.032\n",
            "Loss at step 87: 211.798\n",
            "Loss at step 88: 211.582\n",
            "Loss at step 89: 211.383\n",
            "Loss at step 90: 211.200\n",
            "Loss at step 91: 211.031\n",
            "Loss at step 92: 210.875\n",
            "Loss at step 93: 210.732\n",
            "Loss at step 94: 210.600\n",
            "Loss at step 95: 210.478\n",
            "Loss at step 96: 210.366\n",
            "Loss at step 97: 210.262\n",
            "Loss at step 98: 210.167\n",
            "Loss at step 99: 210.079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vanvD93FV0_k"
      },
      "source": [
        "### Observe values of Weight\n",
        "- Print the updated values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSqpy4gtWaOD",
        "outputId": "1a91e446-53fe-4555-daec-70c4d208f71f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "W"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
              "array([[1.64165  ],\n",
              "       [1.5366124],\n",
              "       [1.6367263],\n",
              "       [1.5182164]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9KpRupYUEwy"
      },
      "source": [
        "### Observe values of Bias\n",
        "- Print the updated values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhEWkGqHWohg",
        "outputId": "1e385bb0-4109-425d-9306-ff42c42f2aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "B"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([3.165002], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jQXNuxvSYqY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}